@article{shin2022exponential,
  title={Exponential Decay of Sensitivity in Graph-Structured Nonlinear Programs},
  author={Shin, Sungho and Anitescu, Mihai and Zavala, Victor M},
  journal={SIAM Journal on Optimization},
  year={2022},
  volume={32},
  number={2},
  pages={1156--1183},
  year={2022},
  publisher={SIAM},
  eprint={arXiv:2101.03067},
	doi={10.1137/21M1391079}
}
@article{jalving2022graph,
  title={A Graph-Based Modeling Abstraction for Optimization: Concepts and Implementation in {P}lasmo.jl.},
  author={Jalving, Jordan and Shin, Sungho and Zavala, Victor M},
  journal={Mathematical Programming Computation},
	doi={10.1007/s12532-022-00223-3},
	year={2022},
  eprint={arXiv:2006.05378}
}
@article{pacaud2023accelerating,
  title={Accelerating condensed interior-point methods on {SIMD}/{GPU} architectures},
  author={Pacaud, Fran{\c{c}}ois and Shin, Sungho and Schanen, Michel and Maldonado, Daniel Adrian and Anitescu, Mihai},
  journal={Journal of Optimization Theory and Applications},
  pages={1--20},
  year={2023},
  eprint = {arXiv:2203.11875},
	doi={10.1007/s10957-022-02129-5},
  publisher={Springer}
}
@article{pacaud2023parallel,
  title={Parallel Interior-Point Solver for Block-Structured Nonlinear Programs on {SIMD}/{GPU} Architectures},
  author={Pacaud, Fran{\c{c}}ois and Schanen, Michel and Shin, Sungho and Maldonado, Daniel Adrian and Anitescu, Mihai},
  journal = {Optimization Methods and Software},
  note={Accepted.},
  eprint={arXiv:2301.04869},
  note={2024}
}
@article{shinAcceleratingOptimalPower2024,
  title = {Accelerating Optimal Power Flow with {{GPUs}}: {{SIMD}} Abstraction of Nonlinear Programs and Condensed-Space Interior-Point Methods},
  shorttitle = {Accelerating Optimal Power Flow with {{GPUs}}},
  author = {Shin, Sungho and Anitescu, Mihai and Pacaud, François},
  date = {2024-11-01},
  journal = {Electric Power Systems Research},
  shortjournal = {Electric Power Systems Research},
  volume = {236},
  pages = {110651},
  issn = {0378-7796},
  doi = {10.1016/j.epsr.2024.110651},
  url = {https://www.sciencedirect.com/science/article/pii/S0378779624005376},
  urldate = {2024-10-21},
  abstract = {This paper introduces a framework for solving alternating current optimal power flow (ACOPF) problems using graphics processing units (GPUs). While GPUs have demonstrated remarkable performance in various computing domains, their application in ACOPF has been limited due to challenges associated with porting sparse automatic differentiation (AD) and sparse linear solver routines to GPUs. We address these issues with two key strategies. First, we utilize a single-instruction, multiple-data abstraction of nonlinear programs. This approach enables the specification of model equations while preserving their parallelizable structure and, in turn, facilitates the parallel AD implementation. Second, we employ a condensed-space interior-point method (IPM) with an inequality relaxation. This technique involves condensing the Karush–Kuhn–Tucker (KKT) system into a positive definite system. This strategy offers the key advantage of being able to factorize the KKT matrix without numerical pivoting, which has hampered the parallelization of the IPM algorithm. By combining these strategies, we can perform the majority of operations on GPUs while keeping the data residing in the device memory only. Comprehensive numerical benchmark results showcase the advantage of our approach. Remarkably, our implementations—MadNLP.jl and ExaModels.jl—running on NVIDIA GPUs achieve an order of magnitude speedup compared with state-of-the-art tools running on contemporary CPUs.},
  keywords = {Automatic differentiation,GPU computing,Nonlinear programming,Optimal power flow},
  file = {/Users/sushin/Zotero/storage/2TBXIKSX/Shin et al. - 2024 - Accelerating optimal power flow with GPUs SIMD abstraction of nonlinear programs and condensed-spac.pdf;/Users/sushin/Zotero/storage/KERN8HXH/S0378779624005376.html}
}

@online{pacaudCondensedspaceMethodsNonlinear2024,
  title = {Condensed-Space Methods for Nonlinear Programming on {{GPUs}}},
  author = {Pacaud, François and Shin, Sungho and Montoison, Alexis and Schanen, Michel and Anitescu, Mihai},
  date = {2024-05-23},
  eprint = {2405.14236},
  eprinttype = {arXiv},
  eprintclass = {math},
  doi = {10.48550/arXiv.2405.14236},
  url = {http://arxiv.org/abs/2405.14236},
  urldate = {2024-05-29},
  abstract = {This paper explores two condensed-space interior-point methods to efficiently solve large-scale nonlinear programs on graphics processing units (GPUs). The interior-point method solves a sequence of symmetric indefinite linear systems, or Karush-Kuhn-Tucker (KKT) systems, which become increasingly ill-conditioned as we approach the solution. Solving a KKT system with traditional sparse factorization methods involve numerical pivoting, making parallelization difficult. A solution is to condense the KKT system into a symmetric positive-definite matrix and solve it with a Cholesky factorization, stable without pivoting. Although condensed KKT systems are more prone to ill-conditioning than the original ones, they exhibit structured ill-conditioning that mitigates the loss of accuracy. This paper compares the benefits of two recent condensed-space interior-point methods, HyKKT and LiftedKKT. We implement the two methods on GPUs using MadNLP.jl, an optimization solver interfaced with the NVIDIA sparse linear solver cuDSS and with the GPU-accelerated modeler ExaModels.jl. Our experiments on the PGLIB and the COPS benchmarks reveal that GPUs can attain up to a tenfold speed increase compared to CPUs when solving large-scale instances.},
  pubstate = {prepublished},
  keywords = {Mathematics - Optimization and Control},
  file = {/Users/sushin/Zotero/storage/N22HHYF6/Pacaud et al. - 2024 - Condensed-space methods for nonlinear programming on GPUs.pdf;/Users/sushin/Zotero/storage/Z33UWWWG/2405.html}
}

